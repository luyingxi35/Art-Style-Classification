\section{Introduction}
In this project, we endeavor to address the problem of classifying paintings according to their artistic styles. The classification of artistic painting styles presents a multifaceted challenge at the intersection of computer vision, machine learning, and art history\cite{CETINIC2018107}. 
In recent years, automated classification of art painting styles using deep convolutional neural networks (CNNs) has become essential for analyzing and categorizing vast digitized art collections, as these models can learn hierarchical visual features directly from raw image data \cite{li2025enhanced}\cite{imran2023artistic}. 
This task requires training systems to accurately recognize and distinguish stylistic characteristics across diverse art movementsâ€”despite challenges such as high intra-class variability and inter-class similarity \cite{alkofer2021using}. 
Developing robust classification methods is critical for scalable digital archiving, enhancing curatorial workflows, and enabling large-scale quantitative analysis of artistic trends to support art historical research.

Our project builds upon the approach proposed in \cite{imran2023artistic}, which involves a two-stage architecture combining a deep neural network (DNN) and a shallow neural network (SNN) adapter. The DNN acts as a feature extractor, while the SNN adapter is responsible for decision making. Each input image is divided into five 
distinct patches: top-left, top-right, bottom-left, bottom-right, and center. The DNN independently classifies each of these five patches, and the SNN adapter aggregates the results to produce a final classification. As suggested in \cite{imran2023artistic}, these architecture allows for a more detailed examination of different regions within an artwork, capturing
fine-grained information and preserving important artistic details. The shallow neural network (SNN) operates independently of the base deep neural network (DNN) model, meaning that the introduction of the SNN does not alter the architecture or weights of the DNN. This independence allows the SNN to function as a flexible adapter, capable of enhancing the classification process without imposing any architectural constraints on the DNN, thereby maintaining the integrity and performance of the original DNN while adding an additional layer of refinement to the classification results.